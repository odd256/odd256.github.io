---
title: 最优化理论
tags:
  - 数学基础
publish: true
created: 2023-02-10T10:41
updated: 2024-02-26T12:38
---

> [!notice]
> 参考书籍：《最优化方法及其 Matlab 程序设计》——马昌凤

# 全篇符号定义

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222175742.png)

# 最优化理论基础

## 最优化问题的数学模型

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230212201906.png)
最优化模型可以由目标函数和限制条件两部分组成

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230212202105.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230212203637.png)
本书主要考虑非线性规划，非线性规划又分为：

1. 无约束优化问题
2. 等式约束优化问题
3. 非等式约束优化问题

还有一些特殊的情况：

1. **二次规划**：目标函数为二次函数而约束函数都是线性函数
2. **线性规划**：目标函数和约束函数都是线性函数

## 向量和矩阵范数

> [!note]
> 详细内容可以参考[数值分析](数值分析.md#向量范数)的内容

范数的条件:

1. $\Vert x \Vert \ge 0, \Vert x \Vert = 0 \Leftrightarrow x= 0$
2. $\Vert \lambda x \Vert = \vert \lambda \vert \Vert x \Vert, \lambda \in \mathbb{R}$
3. $\Vert x+y \Vert \le \Vert x \Vert + \Vert y \Vert$
4. $\Vert AB \Vert \le \Vert A \Vert \Vert B \Vert, A, B \in \mathbb{R}^{n\times n}$
   如果一矩阵范数 $\Vert \cdot \Vert_{\mu}$ 相对于某向量范数 $\Vert \cdot \Vert$ 满足下面的不等式
5. $\Vert Ax \Vert \le \Vert A \Vert_{\mu} \Vert x \Vert, x \in \mathbb{R}^n$
   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230212210630.png)

   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230212205140.png)

常用向量范数：

- 1-范数：$\Vert x \Vert_1 = \sum^{n}_{i=1} \vert x_i \vert$
- 2-范数：$\Vert x \Vert_2 = (\sum_{i=1}^n \vert x_i \vert^2)^{\frac{1}{2}}$
- $\infty$ 范数：$\vert x \vert_{\infty} = \mathop{max} \limits_{1\le i \le n} \vert x_i \vert$
- 说明
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213103102.png)

### 收敛性证明

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213103225.png)

- 向量范数的等价定理
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213104517.png)

- 矩阵范数的等价定理
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213104551.png)

- 收敛性描述
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213105420.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213105429.png)

## 函数的可微性与展开

1. 一阶导数（梯度）
   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213105553.png)
2. 二阶导数（Hesse 矩阵）
   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213105821.png)
   若梯度 $\nabla f(x)$的每个分量函数 在 $x$ 都连续, 则称 𝑓 在 𝑥 一阶连续可微；若 Hesse 阵 $\nabla^2 f(x)$ 的各个分量 函数都连续，则称 $f$ 在 $x$ 二阶连续可微. 若 $f$ 在开集 $D$ 的每一点都连续可微，则称 $f$ 在 $D$ 上一阶连续可微；若 $f$ 在开集 $D$ 的每一点都都二阶连续可微，则称 $f$ 在 $D$ 上二阶连续可微.

> [!note]
> 由上述定义不难发现，若$f$在$x$二阶连续可微，则 Hesse 矩阵是对称阵

### 向量值函数的可微性及中值定理

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213114240.png)

- Jacobi 矩阵的定义
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213114254.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230213115318.png)

- Lipschitz 连续的定义
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230220232203.png)

- 向量值函数的中值定理
  ![屏幕截图 2023-02-20 232630.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-20%20232630.jpg)
  由上面的结论可以得到下面的推论：
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230220234536.png)

## 凸集与凸函数

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230220234618.png)

> [!note]
> 简单来说，凸集就是一个非空集合，**任意**连接其中两点的线段，仍属于该集合

- 凸集的基本性质
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230220234823.png)

- 凸包的定义
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230221001745.png)

- 锥和凸锥的定义
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230221001814.png)

- 凸函数的定义
  ![屏幕截图 2023-02-21 001953.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-21%20001953.jpg)

> [!note]
> 凸函数的定义实际上和之前高数中凸函数的定义（$f(\frac{x+y}{2}) \le \frac{f(x)+f(y)}{2}$）类似，这是$\lambda = 2$的情况，这里使用$\lambda$更一般

- 凸函数的基本性质
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230221002516.png)

- 判断凸性的几种方法
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230221002740.png)
- 二阶导数判断凸性
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222173528.png)
  推广到多元函数上：
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222173603.png)

## 无约束问题的最优性条件

- 极小点的定义
  ![屏幕截图 2023-02-22 175108.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-22%20175108.jpg)

  > [!note]
  > 一般来说，全局最优比较难求，因此通常只求局部极小点

- 一阶必要条件
  ![屏幕截图 2023-02-22 175831.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-22%20175831.jpg)

- 二阶必要条件
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222180532.png)

- 二阶充分条件
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222180616.png)

  > [!note]
  > 一般来说，目标函数的稳定点不一定是极小点。但对于目标函数是凸函数的无约束优化问题，其稳定点、局部极小点和全局极小点三者是等价的。

- 凸函数无约束优化问题——全局极小值点的充要条件
  ![屏幕截图 2023-02-22 180647.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-22%20180647.jpg)
  > [!note]
  > 需要注意和上面区分，凸函数已经表明有最小点，因此一阶必要条件直接变为充分必要条件

## 无约束优化问题的算法框架

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222195420.png)

- 一般算法框架：

  1.  给定初始化参数及初始迭代点 $x_0$ ，置 $k := 0$
  2.  若 $x_k$ 满足某种终止准则, 停止迭代, 以$x_k$作为近似极小点
  3.  通过求解 $x_k$ 处的某个子问题确定下降方向 $d_k$
  4.  通过某种搜索方式确定步长因子 $\alpha_k$ , 使得 $f(x_k+\alpha_k d_k) \lt f(x_k)$
  5.  令 $x_k+1 := x_k + \alpha_k d_k , k := k + 1$, 转步 2

- 下降方向
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222200054.png)
  若目标函数 $f$ 是**一阶连续可微**的, 则判别 $d_k$ 是否为下降方向将有更为方便的判别条件：
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222200110.png)
  ![屏幕截图 2023-02-22 203548.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-22%20203548.jpg)

- 收敛性
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222200718.png)

- 收敛速度
  ![屏幕截图 2023-02-22 201129.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-22%20201129.jpg)

- 终止条件
  ![屏幕截图 2023-02-22 201722.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-22%20201722.jpg)

# 线搜索技术

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222203137.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222205218.png)

线搜索技术分为两种：

1. 精确线搜索

   基本思想：首先确定包含问题最优解的搜索区间，然后采用某种插值或分割技术缩小这个区间，进行搜索求解

   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222204521.png)
   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222204630.png)

   > [!note]
   > 因为这里的$\alpha_k$是**极小值**，因此关于其的导数为$0$

1. 非精确线搜索
   ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222205830.png)

## 搜索区间

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222210117.png)

> [!note]
> 这里强调了极小化问题，及单峰区间/单峰函数的概念，$\alpha^{*}$是一个极小点

### 进退法确定搜索区间

其基本思想是从一点出发，按一定步长，试图确定函数值呈现 “高-低-高”的三点，从而得到一个近似的单峰区间
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230222210816.png)

> [!attention]
> 这个算法出现了笔误，step4 中，应为 $a = min\{\alpha, \alpha_{k+1}\}, b = max\{\alpha, \alpha_{k+1}\}$

## 精确线搜索

精确线搜索分为两类：一类是使用**导数的搜索**，如插值法，牛顿法及抛物线法等；另一类是**不用导数的搜索**，如 0.618 法（黄金分割法），分数法及成功-失败法等，本书仅介绍 0.618 法和二次插值逼近法

### 黄金分割法

黄金分割法也称为 0.618 法，其基本思想是通过试探点函数值得比较，是包含极小点的搜索区间不断缩小。该方法仅需要计算函数值，适用范围广，使用方便

- 公式推导
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223210545.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223210602.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223210631.png)

- 算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223210657.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223210708.png)
  > [!note]
  > 值得说明的是，由于每次迭代搜索区间的收缩率是 $t = 0.618$，故 $0.618$ 法只是线性收敛的，即这一方法的计算效率并不高。但该方法每次迭代只需计算一次函数值的优点足以弥补这一缺憾

### 二次插值法

- 算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223211054.png)

## 非精确线搜索

线搜索技术是求解许多优化问题下降算法的基本组成部分，但精确线搜索往往需要计算很多的函数值和梯度值，从而耗费较多的计算资源。特别是当迭代点远离最优点时，精确线搜索通常不是十分有效和合理的。对于许多优化算法，其收敛速度并不依赖于精确搜索过程。因此，既能保证目标函数具有可接受的下降量又能使最终形成的迭代序列收敛的非精 确线搜索变得越来越流行。本书着重介绍非精确线搜索中的 Wolfe 准则和 Armijo 准则

### Wolfe 准则

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223220243.png)

> [!note]
> 强 Wolfe 准则表明, 由该准则得到的新的迭代点 $x_{k+1} = x_k + \alpha_k d_k$ 在 $x_k$ 的某一邻域内且使目标函数值有一定的下降量

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223220706.png)

### Armijo 准则

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223220743.png)

- 算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223220803.png)

## 线搜索法的收敛性

- 算法步骤
  ![屏幕截图 2023-02-24 161047.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20161047.jpg)

  > [!note] > ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224161423.png)

- 非精确线搜索法的收敛性
  ![屏幕截图 2023-02-24 161646.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20161646.jpg)

- 精确线搜索的收敛性
  ![屏幕截图 2023-02-24 162345.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20162345.jpg)

# 最速下降法和牛顿法

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223212746.png)

## 最速下降法

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223213901.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223213932.png)

> [!attention] > $d_k$是负梯度的方向，但$d_k \ne -\nabla f(x_k)$，需要把梯度改为**单位向量**，因此公式应该变为$d_k=-\frac{\nabla f(x_k)}{\Vert \nabla f(x_k) \Vert}$

- 算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230223213958.png)
  ![屏幕截图 2023-02-24 195230.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20195230.jpg)

- 最速下降法全局收敛定理
  ![屏幕截图 2023-02-24 200049.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20200049.jpg)

  > [!attention] > $d_k$ 只是下降方向，但究竟需要多少步长是由 $\alpha$ 确定的，因此需要**结合线搜索方法**找到合适的$\alpha$

- 收敛速度估计
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224201322.png)
  > [!note]
  > 结论：如果$\kappa$趋近于 $1$，则收敛速度会很快，但若 $H$ 为病态时，算法收敛速度很缓慢

## 牛顿法

跟最速下降法一样，牛顿法也是求解无约束优化问题最早使用的经典算法之一。其基本思想是用迭代点 $x_k$ 处的一阶导数（梯度）和二阶导数（Hesse 阵）对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小点

- 公式推导
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224203502.png)
  > [!note]
  > 在迭代公式（3.4）中每步迭代需要求 Hesse 阵的逆 $G^{-1}_kd=-g_k$，在实际计算中可通过先解 $G_kd = - g_k$ 得 $d_k$ , 然后令 $x_k+1 = x_k + d_k$ 来避免求逆（详细解法可以参考 [Chapter 6. 线性方程组的直接解法](数值分析.md#Chapter%206.%20线性方程组的直接解法)）
- 基本牛顿法的算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224205333.png)
  牛顿法最突出的优点是**收敛速度快**，具有**局部二阶收敛性**

- 阻尼牛顿法的算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224205505.png)

- 全局收敛定理
  ![屏幕截图 2023-02-24 210247.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20210247.jpg)

## 修正牛顿法

从上一节的分析可知，牛顿法具有不低于二阶的收敛速度，这是它的优点。但该算法要求目标函数的 Hesse 阵 $G(x)= \nabla^2 f(x)$ 在每个迭代点 $x_k$ 处是正定的，否则难以保证牛顿方向 $d_k = −G^{−1}_{k} g_k$ 是 $f$ 在 $x_k$ 处的下降方向

修正的途径之一是将牛顿法和最速下降法结合起来，构造所谓的”牛顿-最速下降混合算法“

- 算法步骤
  ![屏幕截图 2023-02-24 210952.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20210952.jpg)

# 共轭梯度法

前面介绍的最速下降法和牛顿法都具有其自身的局限性，本章将要介绍的**共轭梯度法**是**介于最速下降法与牛顿法之间**的一种**无约束优化算法**，它具有**超线性收敛速度**, 而且算法结构简单, 容易编程实现。此外，跟最速下降法相类似，共轭梯度法只用到了**目标函数及其梯度值**，避免了二阶导数( Hesse 阵）的计算，从而降低了计算量和存储量，因此它是求解无约束优化问题的一种比较有效而实用的算法

## 共轭方向法

共轭方向法的基本思想是在求解 $n$ 维正定二次目标函数极小点时产生一组共轭方向作为搜索方向，在精确线搜索条件下算法至多迭代 $n$ 步 即能求得极小点，经过适当的修正后共轭方向法可以推广到求解一般非二次目标函数情形

- 共轭方向
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224212947.png)

  > [!note]
  >
  > - 显然, 向量组的共轭是正交的推广, 即当 $G = I$(单位阵) 时, 上述定义 变成向量组正交的定义
  > - 向量组的共轭是正交的推广，对称正定矩阵 $G$ 的共轭向量组必然是线性无关的

- 算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225161013.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230224214431.png)
  ![屏幕截图 2023-02-24 214445.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-24%20214445.jpg)
  > [!note]
  > 从定理 4.1 可知，在精确线搜索下，用算法 4.1 求解正定二次目标函数极小化问题（4.1），至多在 $n$ 步内即可求得其唯一的极小点。这种能在**有限步内**求得二次函数极小点的性质通常称为**二次终止性**

## 共轭梯度法

共轭梯度法是在每一迭代步利用当前点处的最速下降方向来生成关于凸二次函数 $f$ 的 Hesee 阵 $G$ 的共轭方向，并建立求 $f$ 在 $R^n$ 上的极小点的方法。这一方法最早是由 Hesteness 和 Stiefel 于 1952 年为求解对称正定线性方程组而提出来的，后经 Fletcher 等人研究并应用于无约束优化问题取得了丰富的成果，共轭梯度法也因此成为当前求解无约束优化问题的重要算法类

公式推导：
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225162803.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225162813.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225162821.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225162831.png)

- 最终结论：
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225164750.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225164923.png)

- 算法步骤
  ![屏幕截图 2023-02-25 165025.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20165025.jpg)
  > [!note]
  > 共轭梯度法在精确线搜索和非精确线搜索下都是收敛的
  > ![屏幕截图 2023-02-25 165639.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20165639.jpg)

# 拟牛顿法

第 3 章所介绍的牛顿法的优点是具有二阶收敛速度，但当 Hesse 阵 $G(x_k) = \nabla^2 f(x_k)$ 不正定时，不能保证所产生的方向是目标函数在 $x_k$ 处的下降方向。特别地，当 $G(x_k )$ 奇异时，算法就无法继续进行下去。尽管修正牛顿法可以克服这一缺陷，但其中的修正参数 $\mu_k$ 的选取很难把握，过大或过小都会影响到收敛速度。此外，牛顿法的每一迭代步都需要目标函数的二阶导数，即 Hesee 阵，对于大规模问题其计算量是惊人的

本章即将介绍的拟牛顿法克服了这些缺点，并且在一定条件下这类算法仍然具有较快的收敛速度—超线性收敛速度

## 拟牛顿法及其性质

- 基本思想
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225170515.png)

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225170603.png)
![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225170617.png)

- 对称秩 1 校正公式
  ![屏幕截图 2023-02-25 171341.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20171341.jpg)

- 算法步骤
  ![屏幕截图 2023-02-25 171612.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20171612.jpg)

## BFGS 算法

- 基本思想
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225172154.png)
- 性质
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225172211.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225172246.png)

- 算法步骤
  ![屏幕截图 2023-02-25 172317.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20172317.jpg)

## DFP 算法

- 基本思想
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225172537.png)

- 算法步骤
  ![屏幕截图 2023-02-25 172637.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20172637.jpg)

## Broyden 族算法

![屏幕截图 2023-02-25 173119.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20173119.jpg)

- 性质
  ![屏幕截图 2023-02-25 173354.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20173354.jpg)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225173556.png)

- 算法步骤
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225173621.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225173629.png)

## 拟牛顿法的收敛性

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225173754.png)

- 超线性收敛的充分必要条件：
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230225173831.png)

- BFGS 算法的局部超线性收敛定理：
  ![屏幕截图 2023-02-25 173931.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-25%20173931.jpg)

# 最优性条件

## 等式约束问题的最优性条件

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227165722.png)

- 拉格朗日定理
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227165825.png)

- 二阶充分条件
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227170325.png)

## 不等式约束问题的最优性条件

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227170357.png)

- 非有效约束/有效约束
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227170505.png)

- 基础引理
  ![屏幕截图 2023-02-27 170852.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-27%20170852.jpg)

  > [!note]
  > 这里的**相容**有点像向量的相关性，即$a$可以由$b_i$表示

  ![](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227171651.png)
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227173026.png)

- KT 条件
  ![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230227173037.png)

## 一般约束问题的最优性条件

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230228111412.png)

- KT 一阶必要条件
  ![屏幕截图 2023-02-28 111438.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-28%20111438.jpg)

# 罚函数法

首先介绍求解约束优化问题的经典算法——**罚函数法**。其**基本思想**是：根据约束条件的特点将其转化为某种惩罚函数加到目标函数中去，从而将约束优化问题转化为一系列的无约束优化问题来求解。本章主要介绍**外罚函数法**、**内点法**和**乘子法**

## 外罚函数法

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230228121000.png)
![屏幕截图 2023-02-28 111847.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-28%20111847.jpg)

> [!note]
> 常用的外罚函数有**取平方**、**取绝对值**等
> 需要注意的是$\sigma_k$是一个**很大的值**，用来控制惩罚，从可行域外逐渐逼近可行域
> 优点：
>
> - 外罚函数法结构简单，可以直接调用无约束优化算法的通用程序，因而**容易编程实现**
>   缺点：
> - $x_k$ 往往不是可行点，这对于某些实际问题是难以接受的
> - 罚参数 $\sigma_k$ 的选取比较困难，取的过小，可能起不到“惩罚”的作用，而取得过大则可能造成 $P(x, \sigma_k)$ 的 Hesse 阵的条件数很大，从而带来数值技术上的困难
> - 注意到  $\overline{𝑃}(x)$ 一般是不可微的, 因而难以直接使用利用导数的优化算法，从而收敛速度缓慢

## 内点法

![image.png](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/20230228112408.png)

> [!note]
> 通常可以对函数取倒数之和作为罚函数
> 与外罚函数法==相反==，这里的$\tau$是一个**很小的值**，约束优化问题的极小点一般在可行域的边界上达到
> 优点： - 结构简单, 适应性强
> 缺点： - 随着迭代过程的进行，罚参数 $\tau_{k}$ 将变得越来越小，趋向于零，使得增广目标函数的病态性越来越严重 - 内点法的初始点 $x_0$ 要求是一个严格的可行点，一般来说这也是比较麻烦甚至困难的

## 乘子法

基本思想是从原问题的拉格朗日函数出发，再加上适当的罚函数，从而将原问题转化为求解一系列的无约束优化子问题。由于外罚函数法中的罚参数 $\sigma_k \rightarrow +\infty$，因此增广目标函数变得“越来越病态”。增广目标函数的这种病态性质是外罚函数法的主要缺点，而这种缺陷在乘子法中由于引入拉格朗日函数及加上适当的罚函数而得以有效的克服

![屏幕截图 2023-02-28 120621.jpg](https://obsidian-pic-1258776558.cos.ap-nanjing.myqcloud.com/blog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-02-28%20120621.jpg)
